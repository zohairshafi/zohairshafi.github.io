{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c44d5b",
   "metadata": {},
   "source": [
    "# Homework 2 - Gradient Descent\n",
    "Due Date - Friday 15th February (11:59 PM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fc16cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np  # arrays, vectorized math\n",
    "import pandas as pd  # tabular data handling (not heavily used here but students should familiarize themselves with this)\n",
    "import matplotlib.pyplot as plt  # plotting\n",
    "from sklearn.linear_model import LinearRegression  # ready-made linear regression\n",
    "\n",
    "rng = np.random.default_rng(42)  # reproducible randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a8537",
   "metadata": {},
   "source": [
    "## Synthetic Data\n",
    "We will create noisy linear data so we know the true relationship. \n",
    "Do not modify this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166ff87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic linear regression dataset for optimizer comparisons\n",
    "# - Standardized feature (stable gradients across learning rates)\n",
    "# - Heteroscedastic Gaussian noise that grows with |x|\n",
    "# - Sparse outliers to reveal optimizer robustness\n",
    "\n",
    "n = 1000\n",
    "x_raw = rng.normal(0.0, 10.0, size=n)\n",
    "\n",
    "beta0, beta1 = 4.0, 3.0\n",
    "\n",
    "# Base noise: variance increases with |x|\n",
    "sigma = 2 + 0.5 * np.abs(x_raw)\n",
    "noise = rng.normal(0.0, sigma, size=n)\n",
    "\n",
    "# Inject a few outliers\n",
    "outlier_idx = rng.choice(n, size=int(0.5 * n), replace=False)\n",
    "noise[outlier_idx] += rng.normal(0.0, 8.0, size=outlier_idx.size)\n",
    "\n",
    "# Linear relationship\n",
    "y = beta0 + beta1 * x_raw + noise\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(x_raw, y, s=12, alpha=0.7, label='Training Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Realistic Linear Regression Data')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "# Quick summary\n",
    "print(f'Number of Data Points = {n}\\nMean of X = {x_raw.mean():.3f}\\nStandard Deviation of X = {x_raw.std():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e7c59",
   "metadata": {},
   "source": [
    "# Feature Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c6ea9",
   "metadata": {},
   "source": [
    "# Question 1 (12 Points in total) \n",
    "Normalize the input data using Mean-Variance Normalization and Min-Max Normalization. Print the mean and standard deviation (for Mean-Variance Normalization) and min and max (for Min-Max Normalization) of the normalized data.\n",
    "\n",
    "Min-Max Normalization = $x \\leftarrow \\frac{x - min(x)}{max(x) - min(x)}$\n",
    "\n",
    "\n",
    "Mean Variance Normalization = $x \\leftarrow \\frac{x - mean(x)}{std(x)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fc7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############## Question 1(a) ##################\n",
    "################# 5 points ####################\n",
    "###############################################\n",
    "\n",
    "# TODO: Write your code to to perform min max normalization on x_raw\n",
    "\n",
    "################################################\n",
    "########## Write your answer here ##############    \n",
    "################################################\n",
    "\n",
    "# Use min-max normalization to normalize the input \n",
    "x_min_max = None # Your code here to normalize x_raw\n",
    "\n",
    "################################################\n",
    "########## End of your answer ##################\n",
    "################################################\n",
    "\n",
    "print(f'Number of Data Points = {n}\\nMean of X = {x_min_max.mean():.3f}\\nStandard Deviation of X = {x_min_max.std():.3f}\\nMin of X = {x_min_max.min():.3f}\\nMax of X = {x_min_max.max():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b205e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############## Question 1(b) ##################\n",
    "################# 5 points ####################\n",
    "###############################################\n",
    "\n",
    "# TODO: Write your code to to perform mean-variance normalization on x_raw\n",
    "\n",
    "################################################\n",
    "########## Write your answer here ##############    \n",
    "################################################\n",
    "\n",
    "x_mean_var = None # Your code here to normalize x_raw\n",
    "\n",
    "################################################\n",
    "########## End of your answer ##################\n",
    "################################################\n",
    "\n",
    "print(f'Number of Data Points = {n}\\nMean of X = {x_mean_var.mean():.3f}\\nStandard Deviation of X = {x_mean_var.std():.3f}\\nMin of X = {x_mean_var.min():.3f}\\nMax of X = {x_mean_var.max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ff95cd",
   "metadata": {},
   "source": [
    "## Question 1(c) (2 Points) \n",
    "\n",
    "What difference do you see between min-max and mean-variance normalization? \n",
    "\n",
    "When do you think one would be better than the other? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac42d4",
   "metadata": {},
   "source": [
    "Enter you answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5106ae26",
   "metadata": {},
   "source": [
    "# Question 2 - Gradient Descent Optimizer Implementation (10 Points)\n",
    "\n",
    "Given a linear regression model $\\hat{y} = \\theta_0 + \\theta_1x$ and the mean squared error loss function, the gradients are given by:\n",
    "\n",
    "$\\frac{\\partial L(\\theta)}{\\partial \\theta_0} = \\frac{2}{m}\\sum_{i = 1}^m (\\theta_0 + \\theta_1 x_i - y_i)$\n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{\\partial L(\\theta)}{\\partial \\theta_1} = \\frac{2}{m}\\sum_{i = 1}^m x_i \\cdot (\\theta_0 + \\theta_1 x_i - y_i)$\n",
    "\n",
    "The gradient descent update rule is given by \n",
    "\n",
    "$\\theta_t = \\theta_{t-1} - \\alpha \\cdot \\nabla \\ell_{\\theta_{t-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b417e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the remainder of this code, we will use mean-variance normalized x\n",
    "\n",
    "# Update x to be mean-variance normalized version\n",
    "x = x_mean_var\n",
    "\n",
    "# Generate training data again for consistency\n",
    "y = beta0 + beta1 * x + noise\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(x_raw, y, s=12, alpha=0.7, label='Training Data')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Realistic Linear Regression Data')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119710d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Compute the Mean Squared Error between true and predicted values.\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "###############################################\n",
    "############### Question 2 ####################\n",
    "###############################################\n",
    "\n",
    "# TODO: We're going to use gradient descent to fit a linear model to this data.\n",
    "# Fill in the code below using the equations defined above. \n",
    "\n",
    "################################################\n",
    "########## Write your answer here ##############    \n",
    "################################################\n",
    "\n",
    "\n",
    "def run_gradient_descent(x, y, learning_rate = 1e-4, epochs=1000):\n",
    "    \"\"\"Run gradient descent to fit a simple linear regression model.\"\"\"\n",
    "\n",
    "    # Initialize parameters\n",
    "    # We will initialize both to zero for now \n",
    "    # This will need to change if we're dealing with neural networks later\n",
    "    # but works fine for linear regression for now \n",
    "    theta_0_gd = 0.0\n",
    "    theta_1_gd = 0.0\n",
    "\n",
    "    # Run gradient descent for simple linear regression\n",
    "\n",
    "    # Get number of rows of data \n",
    "    m = len(x)\n",
    "\n",
    "    # A list to track loss over iterations\n",
    "    loss_history = []\n",
    "\n",
    "    # Iterate over the data set - remember, one epoch is one full pass over the dataset \n",
    "    for e in range(epochs):\n",
    "\n",
    "        # Define the model equation \n",
    "        y_pred = None # Enter your code here\n",
    "\n",
    "\n",
    "        # Gradients for intercept and slope \n",
    "        # Use the equations above to fill in the following lines\n",
    "        # Hint: x is a vector. So is y. So you can use np.sum() to sum the error over all data points \n",
    "        grad_theta_0 = None # Enter your code here\n",
    "        grad_theta_1 = None # Enter your code here\n",
    "        \n",
    "        # Update parameters\n",
    "        # Use the gradient descent update equations to fill in the following lines\n",
    "        theta_0_gd = None # Enter your code here\n",
    "        theta_1_gd = None # Enter your code here\n",
    "\n",
    "        ################################################\n",
    "        ########## End of your answer ##################\n",
    "        ################################################\n",
    "\n",
    "        if e % 50 == 0 or e == epochs - 1:\n",
    "            loss_history.append(mean_squared_error(y, y_pred))\n",
    "\n",
    "    print (\"==========================\")\n",
    "    print (\"Learning Rate:\", learning_rate)\n",
    "    print(f\"Slope (theta_1): {theta_1_gd:.3f}\")\n",
    "    print(f\"Intercept (theta_0): {theta_0_gd:.3f}\")\n",
    "    print(f\"MSE: {mean_squared_error(y, theta_0_gd + theta_1_gd * x):.3f}\")\n",
    "    print (\"==========================\")\n",
    "    print ()\n",
    "    \n",
    "    return (theta_0_gd, theta_1_gd), np.arange(0, epochs, 50).tolist() + [epochs - 1], loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e75fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fit and loss curve\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "\n",
    "for lr in [1, 1e-1, 1e-2, 1e-3, 1e-4, 1e-5]:\n",
    "    (theta_0_gd, theta_1_gd), iter_history, loss_history = run_gradient_descent(x, y, learning_rate=lr, epochs=1000)\n",
    "    ax1.plot(np.sort(x), (theta_0_gd + theta_1_gd * np.sort(x)), label=\"Learned Model - Learning Rate: {:.0e}\".format(lr))\n",
    "    ax2.plot(np.array(iter_history), loss_history, marker=\"o\", label=f\"LR: {lr:.6f}\")\n",
    "\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.scatter(x, y, s=12, alpha=0.7, label='Training Data')\n",
    "\n",
    "ax1.set_title(\"Gradient Descent Fit\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_xlabel(\"Training Iteration\")\n",
    "ax2.set_ylabel(\"Mean Squared Error\")\n",
    "ax2.set_title(\"Loss During Training\")\n",
    "ax2.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3d269",
   "metadata": {},
   "source": [
    "# Question 3 (4 points - 2 each)\n",
    "\n",
    "3(a) What happens as you change the learning rate? What happens when the learning rate is high? What about when it is very small? \n",
    "\n",
    "3(b) Which learning rate seems optimal and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3bcb28",
   "metadata": {},
   "source": [
    "Enter your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646addc7",
   "metadata": {},
   "source": [
    "## Question 4 (10 points) - Implement gradient descent with momentum \n",
    "\n",
    "If the gradient descent equation is given by $\\theta_t = \\theta_{t-1} - \\alpha \\nabla \\ell_{\\theta_{t-1}}$, then the momentum modified variant is given by \n",
    "\n",
    "$v_t = \\beta \\cdot v_{t-1} + \\nabla \\ell_{\\theta_{t-1}}$\n",
    "\n",
    "$\\theta_t = \\theta_{t-1} - \\alpha \\cdot v_{t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d9d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############### Question 4 ####################\n",
    "###############################################\n",
    "\n",
    "# TODO: Use your gradient descent code from above and modify it to implement Momentum Gradient Descent.\n",
    "\n",
    "################################################\n",
    "########## Write your answer here ##############    \n",
    "################################################\n",
    "\n",
    "# Implement Momentum Gradient Descent\n",
    "def run_gradient_descent_momentum(x, y, learning_rate, epochs, beta=0.9):\n",
    "\n",
    "    # Run gradient descent for linear regression (matrix form) with momentum\n",
    "    loss_history = []\n",
    "    iter_history = []\n",
    "\n",
    "    # Initialize velocity\n",
    "    # We can set initial velocities to zero\n",
    "    # (think of a ball rolling down a hill starting from rest with zero initial velocity)\n",
    "    v_0 = 0.0 # Velocity for theta_0\n",
    "    v_1 = 0.0 # Velocity for theta_1\n",
    "\n",
    "    # Intialize parameters\n",
    "    theta_0_gd = 0.0\n",
    "    theta_1_gd = 0.0\n",
    "\n",
    "    # Get number of rows of data\n",
    "    m = len(x)\n",
    "\n",
    "    # Run gradient descent for  linear regression\n",
    "    for t in range(epochs):\n",
    "\n",
    "        # Define the model equation \n",
    "        y_pred = None # Enter your code here from the previous quesion \n",
    "\n",
    "        \n",
    "        # Gradients for intercept and slope\n",
    "        grad_0 = None # Enter your code here from the previous quesion - gradient for theta_0\n",
    "        grad_1 = None # Enter your code here from the previous quesion - gradient for theta_1\n",
    "\n",
    "        # Update velocities \n",
    "        # Use the momentum update equations to fill in the following lines\n",
    "        v_0 = None # Your code here to update v_0 for theta_0\n",
    "        v_1 = None # Your code here to update v_1 for theta_1\n",
    "        \n",
    "        # Update parameters\n",
    "        # Use the momentum update equations to fill in the following lines\n",
    "        # Your code should look similar to what you answered in the previous question\n",
    "        # but make sure to replace the gradient with the velocity terms\n",
    "        theta_0_gd = None # Your code here to update theta_0_gd\n",
    "        theta_1_gd = None # Your code here to update theta_1_gd\n",
    "\n",
    "        ################################################\n",
    "        ########## End of your answer ##################\n",
    "        ################################################\n",
    "\n",
    "        # Track loss periodically\n",
    "        if t % 50 == 0 or t == epochs - 1:\n",
    "            loss_history.append(np.mean(error ** 2))\n",
    "            iter_history.append(t)\n",
    "            \n",
    "    return (theta_0_gd, theta_1_gd), np.arange(0, epochs, 50).tolist() + [epochs - 1], loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcdbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fit and loss curve\n",
    "lr = 1e-3\n",
    "for beta in [0, 0.5, 0.99, 0.999]:\n",
    "    (theta_0_gd, theta_1_gd), iter_history, loss_history = run_gradient_descent_momentum(x, y, learning_rate=lr, epochs=1000, beta=beta)\n",
    "    plt.plot(np.array(iter_history), loss_history, marker=\"o\", label=f\"Beta: {beta:.4f}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss Across Momentum Betas\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4270a3",
   "metadata": {},
   "source": [
    "## Question 5 (6 points - 2 each)\n",
    "\n",
    "5(a): What happens when $\\beta = 0$?\n",
    "\n",
    "5(b): What happens when we set a high value for $\\beta$ at $\\beta = 0.999$?\n",
    "\n",
    "5(c): What seems like an optimal $\\beta$ and why? Does this align with the expectation of what beta is supposed to do? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f13c019",
   "metadata": {},
   "source": [
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac4d7d8",
   "metadata": {},
   "source": [
    "## Question 6 (10 points) - Implement gradient descent with RMSProp \n",
    "\n",
    "If the gradient descent equation is given by $\\theta_t = \\theta_{t-1} - \\alpha \\nabla \\ell_{\\theta_{t-1}}$, then the RMSProp modified variant is given by \n",
    "\n",
    "$G_t = \\beta \\cdot G_{t - 1} + (1 - \\beta) \\cdot (\\nabla \\ell_{\\theta_{t-1}})^2$\n",
    "\n",
    "and\n",
    "\n",
    "$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t} + \\epsilon} \\cdot \\nabla \\ell_{\\theta_{t-1}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e7d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "############### Question 6 ####################\n",
    "###############################################\n",
    "\n",
    "# TODO: Use your gradient descent code from above and modify it to implement RMSProp Gradient Descent.\n",
    "\n",
    "################################################\n",
    "########## Write your answer here ##############    \n",
    "################################################\n",
    "\n",
    "# Implement RMSProp Gradient Descent\n",
    "def run_gradient_descent_rmsprop(x, y, learning_rate, epochs, beta=0.9, epsilon=1e-8):\n",
    "\n",
    "    # Define epsilon for numerical stability\n",
    "    epsilon = 1e-6\n",
    "    \n",
    "    # Run gradient descent for linear regression (matrix form) with RMSProp\n",
    "    loss_history = []\n",
    "    iter_history = []\n",
    "\n",
    "    # Initialize tracking terms for RMSProp\n",
    "    # These terms accumulate squared gradients\n",
    "    # and go in the denominator of the update equations\n",
    "    # Initialize to 1.0 to avoid division by zero in the first iteration\n",
    "    # Hint: If you are confused about 7(a), write out the update equations with the initial values = 1.0 as \n",
    "    # defined below.\n",
    "    g_0 = 1.0 # Tracking term for theta_0\n",
    "    g_1 = 1.0 # Tracking term for theta_1\n",
    "\n",
    "    # Intialize parameters\n",
    "    theta_0_gd = 0.0\n",
    "    theta_1_gd = 0.0\n",
    "\n",
    "    m = len(x)\n",
    "\n",
    "    # Run gradient descent for  linear regression\n",
    "    for t in range(epochs):\n",
    "\n",
    "        # Define the model equation \n",
    "        y_pred = None # Enter your code here from the previous quesion\n",
    "\n",
    "        \n",
    "        # Gradients for intercept and slope \n",
    "        grad_0 = None # Enter your code here from the previous quesion - gradient for theta_0\n",
    "        grad_1 = None # Enter your code here from the previous quesion - gradient for theta_1\n",
    "\n",
    "        # Update tracking terms for RMSProp\n",
    "        g_0 = None # Your code here to update g_0\n",
    "        g_1 = None # Your code here to update g_1\n",
    "        \n",
    "        # Update parameters - be careful with parentheses here. Make sure you dont \n",
    "        # accidentally divide or multiply the wrong things due to parentheses placement\n",
    "        # Also make sure to add epsilon in the denominator for numerical stability. \n",
    "        theta_0_gd = None # Your code here to update theta_0_gd\n",
    "        theta_1_gd = None # Your code here to update theta_1_gd\n",
    "\n",
    "        ################################################\n",
    "        ########## End of your answer ##################\n",
    "        ################################################\n",
    "\n",
    "        # Track loss periodically\n",
    "        if t % 50 == 0 or t == epochs - 1:\n",
    "            loss_history.append(np.mean(error ** 2))\n",
    "            iter_history.append(t)\n",
    "            \n",
    "    return (theta_0_gd, theta_1_gd), np.arange(0, epochs, 50).tolist() + [epochs - 1], loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0479c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot fit and loss curve\n",
    "lr = 1e-3\n",
    "for beta in [0, 0.99, 0.999, 0.999999]:\n",
    "    (theta_0_gd, theta_1_gd), iter_history, loss_history = run_gradient_descent_rmsprop(x, y, learning_rate=lr, epochs=1000, beta=beta)\n",
    "    plt.plot(np.array(iter_history), loss_history, marker=\"o\", label=f\"Beta: {beta:.3f}\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"Training Loss Across RMSProp Betas\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05548868",
   "metadata": {},
   "source": [
    "## Question 7 - (8 points - 4 each)\n",
    "\n",
    "7(a): What is the difference between $\\beta=0$ and $\\beta=1$ in RMSProp? It might help if you write out the equation and replace $\\beta$ with 0 and 1.\n",
    "\n",
    "7(b): Does the answer to 7(a) correspond to what you see in the plot above? In terms of the plot, what is the main difference between the red and the blue lines? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331de5d",
   "metadata": {},
   "source": [
    "Enter your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacba19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54560898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f968ef5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b1401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0c70e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52644567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c3ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
